# TCGA Multi-modal Cancer Classification

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![W&B](https://img.shields.io/badge/Weights%20%26%20Biases-Experiments-yellow)](https://wandb.ai/linaomar1016-university-of-amsterdam/tcga-multimodal)

A reproducible MLOps pipeline for multi-modal cancer classification using TCGA TITAN visual embeddings and clinical text reports processed through vLLM.

## ğŸ¯ Project Overview

This project classifies **32 cancer types** from The Cancer Genome Atlas (TCGA) dataset using:
- **Visual modality**: Pre-extracted TITAN embeddings (768-dim) from Whole Slide Images
- **Text modality**: Clinical reports cleaned with Qwen3-4B and embedded with Qwen3-Embedding-0.6B (1024-dim)

## ğŸ“Š Results

| Model | Test Macro-F1 | Test Micro-F1 | Accuracy |
|-------|---------------|---------------|----------|
| Visual Only (TITAN) | 0.57 | 0.69 | 69% |
| Text Only (Qwen3) | 0.84 | 0.88 | 88% |
| **Multi-modal (Late Fusion)** | **0.85** | **0.90** | **90%** |

ğŸ“ˆ [View all experiments on W&B](https://wandb.ai/linaomar1016-university-of-amsterdam/tcga-multimodal)

## ğŸ“ Repository Structure
```
tcga-multimodal-mlops/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml              # Centralized configuration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/               # Text embeddings
â”‚   â””â”€â”€ splits/                  # Patient-aware splits
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ models/                  # Trained models (best_model.pt)
â”‚   â”œâ”€â”€ logs/                    # Training logs
â”‚   â””â”€â”€ figures/                 # Visualizations
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                 # Training script
â”‚   â”œâ”€â”€ inference.py             # Inference script
â”‚   â””â”€â”€ visualize_embeddings.py  # UMAP/t-SNE visualization
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                    # Data loading and preprocessing
â”‚   â”œâ”€â”€ models/                  # Model architectures
â”‚   â”œâ”€â”€ training/                # Training pipeline
â”‚   â””â”€â”€ utils/                   # Config, visualization utilities
â”œâ”€â”€ slurm_jobs/                  # SLURM job scripts for Snellius
â”œâ”€â”€ tcga_data/                   # Raw TCGA data
â””â”€â”€ vllm/                        # vLLM scripts for text processing
```

## ğŸš€ Quick Start

### Installation
```bash
git clone https://github.com/Linaomaruni/tcga-multimodal-mlops.git
cd tcga-multimodal-mlops
pip install -r requirements.txt
```

### Training
```bash
# Train multimodal model with W&B logging
python scripts/train.py --model_type multimodal --wandb_project tcga-multimodal

# Train with custom hyperparameters
python scripts/train.py --lr 0.0005 --dropout 0.4 --hidden_dim 512
```

### Inference
```bash
python scripts/inference.py --model_path outputs/models/best_model.pt
```

## ğŸ”¬ Methodology

### Data Pipeline
1. **Data Inspection**: 7,544 patients with complete data across 32 cancer types
2. **Patient-Aware Splitting**: 70/15/15 train/val/test split with no patient overlap
3. **Text Cleaning**: vLLM decoder (Qwen3-4B-AWQ) cleans raw pathology reports
4. **Text Embedding**: vLLM encoder (Qwen3-Embedding-0.6B) generates 1024-dim embeddings

### Model Architecture
Late-fusion MLP combining:
- Visual branch: 768 â†’ 512 (TITAN embeddings)
- Text branch: 1024 â†’ 512 (Qwen3 embeddings)
- Fusion: Concatenate â†’ 512 â†’ 256 â†’ 32 classes

### Hyperparameter Optimization
5 configurations tested with W&B tracking:
- Learning rates: [0.001, 0.0005]
- Dropout: [0.3, 0.4]
- Hidden dimensions: [256, 512]

## ğŸ“ˆ Visualizations

### Latent Space Analysis
![t-SNE](outputs/figures/tsne_embeddings.png)
![UMAP](outputs/figures/umap_embeddings.png)

### Confusion Matrix
![Confusion Matrix](outputs/figures/test_confusion_matrix.png)

## ğŸ–¥ï¸ Snellius HPC

### vLLM Text Processing
```bash
cd vllm/src
sbatch slurm_jobs/tcga_decoder.job  # Clean reports 
sbatch slurm_jobs/tcga_encoder.job  # Generate embeddings 
```

### Training
```bash
sbatch slurm_jobs/train.job
```

## ğŸ‘¥ Authors

- Lina Omar 15862984
- Sarah Schaefers 15519503
- Fien van Engelen  14655179
- Jette Walvis 15834468
- Carmen van der Lans 15715353

## ğŸ“„ License

MIT License